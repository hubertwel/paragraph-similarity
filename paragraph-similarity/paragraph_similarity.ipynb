{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paragraph_similarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Cpzo7rPDVpGRiYAGuvjvEmJGO9IcQELh",
      "authorship_tag": "ABX9TyMTL/LT+nuPchA9FZsZErvX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hubertwel/paragraph-similarity/blob/main/paragraph-similarity/paragraph_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJr_9dXGpJ05",
        "outputId": "739bb5d6-02d0-403a-b0d2-22b05a8c9f64"
      },
      "source": [
        "# UNDER CONSTRUCTION\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from gensim.parsing.preprocessing import strip_tags, strip_multiple_whitespaces\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_short\n",
        "from gensim.parsing.preprocessing import strip_non_alphanum, split_alphanum\n",
        "from gensim.parsing.preprocessing import strip_punctuation\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.utils import RULE_KEEP, RULE_DISCARD, RULE_DEFAULT\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import os\n",
        "import smart_open\n",
        "import csv\n",
        "import collections\n",
        "import random\n",
        "import re\n",
        "import optuna\n",
        "\n",
        "def read_corpus(fname, tokens_only=False):\n",
        "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
        "        csv_reader = csv.DictReader(f, quoting=csv.QUOTE_ALL)\n",
        "        #CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short, strip_non_alphanum, split_alphanum]\n",
        "        CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_multiple_whitespaces]\n",
        "        for i, row in enumerate(csv_reader):\n",
        "          line = row['x']\n",
        "          line = remove_urls(line)\n",
        "          line_list = preprocess_string(line, CUSTOM_FILTERS)\n",
        "          line = \" \".join(line_list)\n",
        "          tokens = simple_preprocess(line)\n",
        "          if tokens_only:\n",
        "            yield tokens\n",
        "          else:\n",
        "            # For training data, add tags\n",
        "            yield TaggedDocument(tokens, [i])\n",
        "\n",
        "def remove_urls(text):\n",
        "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', text, flags=re.MULTILINE)\n",
        "  return text\n",
        "\n",
        "def trim_rule(word, count, min_count):\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    # This rule is only used to prune vocabulary during the current method call.\n",
        "    # It is better to print docs with stopwords and with words of any length.\n",
        "    if ((word in stop_words) or (len(word) < 3)):\n",
        "        return utils.RULE_DISCARD  # throw out\n",
        "    else:\n",
        "        return utils.RULE_DEFAULT  # apply default rule, i.e. min_count\n",
        "\n",
        "# Define an objective function to be maximized\n",
        "def objective(trial):\n",
        "  classifier_name = trial.suggest_categorical(\"classifier\", [\"LogReg\"])\n",
        "  # Setup values for the hyperparameters:\n",
        "  if classifier_name == 'LogReg':\n",
        "    logreg_c = trial.suggest_float(\"logreg_c\", 1e-10, 1e10, log=True)\n",
        "    clf = linear_model.LogisticRegression(C=logreg_c)\n",
        "  # Scoring method:\n",
        "    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    score = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=-1, scoring='accuracy')\n",
        "    accuracy = score.mean()\n",
        "    return accuracy\n",
        "    \n",
        "# Set file names for train and test data\n",
        "# test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
        "test_data_dir = os.path.join('/content/drive/MyDrive/Colab Notebooks/data/', 'gouvfr', 'CorpusRandomTwitter')\n",
        "print(\"test_data_dir: %s\" % test_data_dir)\n",
        "train_file = os.path.join(test_data_dir, 'randomtweets3.txt')\n",
        "test_file = os.path.join(test_data_dir, 'randomtweets4.txt')\n",
        "\n",
        "train_corpus = list(read_corpus(train_file, tokens_only=True))\n",
        "train_corpus_tagged = list(read_corpus(train_file))\n",
        "test_corpus = list(read_corpus(test_file, tokens_only=True))\n",
        "\n",
        "print('train_corpus the beginning: ', train_corpus[:2])\n",
        "print('train_corpus length %d' % len(train_corpus))\n",
        "print('train_corpus_tagged the beginning: ', train_corpus_tagged[:2])\n",
        "print('test_corpus the beginning: ', test_corpus[:2])\n",
        "print('test_corpus length %d' %len(test_corpus))\n",
        "print()\n",
        "\n",
        "# Build a vocabulary\n",
        "model = Doc2Vec(dm=0, vector_size=80, min_count=3, epochs=50, hs=1, dbow_words=1, trim_rule=trim_rule)\n",
        "model.build_vocab(train_corpus_tagged)\n",
        "\n",
        "# Train the model on the corpus\n",
        "model.train(train_corpus_tagged, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "print()\n",
        "\n",
        "# Assessing the model\n",
        "ranks = []\n",
        "first_ranks = []\n",
        "second_ranks = []\n",
        "inferred_vectors = []\n",
        "errors = 0\n",
        "for doc_id in range(len(train_corpus_tagged)):\n",
        "    inferred_vector = model.infer_vector(train_corpus_tagged[doc_id].words)\n",
        "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "    # sanity check (self-similarity)\n",
        "    rank = [docid for docid, sim in sims].index(doc_id)\n",
        "    ranks.append(rank)\n",
        "    first_ranks.append(sims[0][0])\n",
        "    inferred_vectors.append(inferred_vector)\n",
        "        \n",
        "print('sims: ', sims[:3])\n",
        "print('ranks: ', ranks)\n",
        "counter = collections.Counter(ranks)\n",
        "print(counter)\n",
        "print()\n",
        "\n",
        "# Testing the model with one random document\n",
        "# Pick a random document from the test corpus and infer a vector from the model\n",
        "doc_id = random.randint(0, len(test_corpus) - 1)\n",
        "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
        "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "print('RANDOM TEST DOCUMENT ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus_tagged[doc_id].words)))\n",
        "\n",
        "# Compare and print the most/second-most/third-most/median/least similar documents from the train corpus\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('THIRD-MOST', 2), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus_tagged[sims[index][0]].words)))\n",
        "\n",
        "filename = get_tmpfile(\"parsim_doc2vec_model\")\n",
        "model.save(filename)\n",
        "\n",
        "# Assessing the model\n",
        "ranks_test = []\n",
        "first_ranks_test = []\n",
        "inferred_vectors_test = []\n",
        "for doc_id in range(len(test_corpus)):\n",
        "    inferred_vector_test = model.infer_vector(test_corpus[doc_id])\n",
        "    sims_test = model.dv.most_similar([inferred_vector_test], topn=len(model.dv))\n",
        "    # sanity check (self-similarity)\n",
        "    rank_test = [docid for docid, sim in sims_test].index(doc_id)\n",
        "    ranks_test.append(rank_test)\n",
        "    first_ranks_test.append(sims_test[0][0])\n",
        "    inferred_vectors_test.append(inferred_vector_test)\n",
        "\n",
        "print('first_ranks_test: ', first_ranks_test)\n",
        "\n",
        "# Cross validatiom\n",
        "tags_array_train = np.array(first_ranks)\n",
        "vectors_2Darray_train = np.array(inferred_vectors)\n",
        "tags_array_test = np.array(first_ranks_test)\n",
        "vectors_2Darray_test = np.array(inferred_vectors_test)\n",
        "y_train, X_train = tags_array_train, vectors_2Darray_train\n",
        "y_test, X_test = tags_array_test, vectors_2Darray_test\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "clf = LogisticRegression(solver='liblinear', max_iter=300, class_weight='balanced', multi_class='auto')\n",
        "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "score = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=-1, scoring='accuracy')\n",
        "print('score: ', score)\n",
        "print('Validation accuracy: {}'.format(round(np.mean(score)*100, 3)))\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Test accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_data_dir: /content/drive/MyDrive/Colab Notebooks/data/gouvfr/CorpusRandomTwitter\n",
            "train_corpus the beginning:  [['rt', 'americanidol', 'watch', 'as', 'ddlovato', 'gives', 'an', 'amazing', 'performance', 'of', 'her', 'single', 'stonecold', 'demionidol'], ['rt', 'dipti_varun', 'varun', 'dhawan', 'junaid', 'is', 'rookie', 'cop', 'and', 'heâ', 'kind', 'of', 'discovering', 'himself', 'as', 'the', 'movie', 'goes', 'on', 'varun_dvn', 'dishoom']]\n",
            "train_corpus length 1000\n",
            "train_corpus_tagged the beginning:  [TaggedDocument(words=['rt', 'americanidol', 'watch', 'as', 'ddlovato', 'gives', 'an', 'amazing', 'performance', 'of', 'her', 'single', 'stonecold', 'demionidol'], tags=[0]), TaggedDocument(words=['rt', 'dipti_varun', 'varun', 'dhawan', 'junaid', 'is', 'rookie', 'cop', 'and', 'heâ', 'kind', 'of', 'discovering', 'himself', 'as', 'the', 'movie', 'goes', 'on', 'varun_dvn', 'dishoom'], tags=[1])]\n",
            "test_corpus the beginning:  [['live', 'morning', 'news', 'weather', 'and', 'traffic', 'updates', 'fromâ', 'wdsu'], ['when', 'catch', 'fire', 'and', 'watch', 'over', 'you', 'like', 'the', 'sun']]\n",
            "test_corpus length 1000\n",
            "\n",
            "\n",
            "sims:  [(807, 0.9646302461624146), (999, 0.9642543792724609), (677, 0.9596692323684692)]\n",
            "ranks:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 15, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 12, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 10, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 9, 0, 0, 0, 0, 16, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 10, 0, 0, 0, 0, 0, 20, 0, 0, 11, 0, 0, 2, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 24, 0, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 16, 0, 0, 0, 0, 0, 0, 22, 0, 0, 0, 2, 0, 0, 15, 0, 0, 0, 28, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 33, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 27, 0, 0, 0, 0, 1, 24, 0, 31, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 1, 8, 0, 1, 0, 0, 0, 0, 7, 0, 1, 0, 0, 0, 0, 0, 1, 16, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "Counter({0: 886, 1: 46, 2: 8, 4: 7, 15: 5, 23: 5, 7: 4, 16: 4, 24: 4, 3: 3, 22: 2, 20: 2, 19: 2, 6: 2, 12: 2, 10: 2, 9: 2, 8: 2, 11: 2, 31: 2, 5: 2, 30: 1, 26: 1, 28: 1, 33: 1, 21: 1, 27: 1})\n",
            "\n",
            "RANDOM TEST DOCUMENT (378): «bloodymellark ron and hermione»\n",
            "\n",
            "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow+w,d80,n5,hs,w5,mc3,s0.001,t3):\n",
            "\n",
            "MOST (131, 0.8967499136924744): «rt autosport gp announces supporting calendar including baku and return of sepang»\n",
            "\n",
            "SECOND-MOST (617, 0.694669246673584): «rt artofliving miracle of enzymes yamunaiscleaner»\n",
            "\n",
            "THIRD-MOST (720, 0.666295051574707): «rt readersgazette puritan witch the redemption of rebecca eames penijorenner»\n",
            "\n",
            "MEDIAN (885, 0.3390335738658905): «chrisariza hi chrisariza thanks for following what aspect of fitness are you interested in»\n",
            "\n",
            "LEAST (436, 0.14480358362197876): «rt twskeptic sargon_of_akkad if the software was bad then you think there would be an apology about it notice the lacâ»\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-02-14 00:54:35,851]\u001b[0m A new study created in memory with name: no-name-8ea7681e-e74a-4dcf-bfd0-f42654a3d763\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "first_ranks_test:  [139, 763, 530, 492, 994, 450, 349, 529, 238, 131, 676, 697, 814, 656, 662, 693, 582, 168, 87, 77, 142, 893, 208, 412, 340, 82, 312, 50, 507, 235, 786, 36, 647, 42, 788, 7, 145, 496, 124, 177, 545, 893, 498, 914, 485, 355, 671, 367, 76, 82, 435, 127, 535, 281, 194, 68, 413, 298, 389, 789, 621, 567, 82, 939, 978, 545, 845, 240, 738, 251, 781, 764, 96, 270, 22, 692, 285, 76, 131, 546, 994, 738, 399, 336, 503, 82, 553, 844, 647, 709, 87, 733, 221, 483, 768, 463, 131, 481, 147, 778, 634, 242, 131, 17, 149, 103, 141, 612, 657, 514, 791, 435, 95, 163, 229, 921, 117, 994, 78, 769, 647, 886, 700, 562, 314, 340, 131, 131, 742, 764, 974, 788, 165, 9, 576, 352, 557, 683, 438, 660, 292, 572, 775, 722, 715, 764, 121, 962, 201, 859, 607, 131, 845, 117, 659, 974, 277, 79, 87, 934, 872, 682, 399, 811, 352, 993, 259, 76, 414, 397, 621, 261, 964, 364, 720, 278, 992, 360, 775, 967, 628, 507, 812, 591, 199, 854, 909, 127, 426, 571, 452, 32, 764, 208, 163, 170, 770, 604, 430, 369, 795, 387, 680, 986, 514, 601, 100, 106, 834, 500, 740, 976, 95, 3, 226, 398, 840, 918, 217, 278, 683, 878, 716, 352, 89, 194, 847, 888, 17, 223, 168, 859, 886, 91, 818, 615, 473, 163, 894, 678, 268, 577, 204, 418, 912, 492, 732, 886, 874, 553, 936, 323, 814, 187, 287, 459, 550, 42, 407, 131, 764, 541, 133, 120, 680, 486, 69, 145, 458, 551, 698, 835, 150, 322, 131, 893, 3, 15, 966, 745, 876, 273, 195, 351, 949, 640, 85, 758, 417, 269, 29, 769, 734, 380, 496, 958, 437, 607, 570, 234, 234, 900, 234, 104, 151, 805, 955, 105, 131, 459, 553, 445, 760, 851, 722, 305, 964, 773, 573, 864, 208, 36, 743, 741, 893, 422, 600, 982, 292, 208, 861, 374, 182, 769, 235, 611, 131, 11, 594, 970, 974, 131, 634, 99, 305, 877, 566, 233, 28, 961, 261, 42, 103, 437, 222, 496, 578, 167, 151, 660, 660, 568, 292, 297, 95, 90, 288, 242, 490, 682, 392, 95, 455, 568, 643, 964, 435, 635, 131, 1, 572, 710, 886, 351, 342, 771, 125, 659, 320, 557, 19, 191, 546, 420, 670, 659, 476, 188, 577, 291, 340, 783, 604, 909, 272, 671, 631, 945, 733, 708, 390, 964, 453, 298, 964, 992, 32, 836, 342, 986, 51, 42, 754, 625, 512, 503, 886, 330, 695, 611, 764, 150, 900, 407, 131, 529, 513, 252, 809, 352, 169, 827, 660, 215, 469, 342, 453, 103, 188, 25, 919, 51, 558, 776, 733, 494, 250, 660, 463, 900, 261, 809, 103, 5, 710, 516, 841, 42, 405, 399, 811, 813, 483, 895, 735, 459, 657, 127, 131, 551, 476, 657, 213, 111, 126, 954, 481, 744, 307, 63, 695, 859, 320, 251, 987, 647, 439, 103, 340, 174, 955, 722, 772, 282, 885, 292, 619, 454, 82, 488, 699, 657, 861, 659, 859, 166, 981, 814, 164, 117, 189, 510, 41, 618, 886, 127, 55, 676, 12, 790, 609, 920, 570, 924, 747, 905, 927, 818, 320, 103, 886, 986, 696, 614, 257, 256, 341, 493, 819, 216, 970, 94, 11, 351, 501, 578, 843, 37, 763, 292, 437, 966, 131, 769, 131, 719, 689, 625, 710, 886, 405, 641, 967, 87, 516, 647, 861, 851, 259, 679, 637, 761, 550, 441, 803, 573, 972, 443, 270, 28, 364, 885, 496, 167, 193, 761, 822, 722, 551, 399, 369, 326, 894, 258, 697, 608, 483, 809, 44, 886, 328, 321, 575, 89, 886, 423, 427, 467, 733, 131, 837, 656, 682, 789, 213, 139, 929, 143, 87, 125, 574, 9, 3, 177, 727, 214, 918, 707, 131, 723, 886, 160, 591, 617, 131, 740, 257, 378, 454, 551, 677, 965, 428, 662, 202, 79, 651, 917, 131, 508, 754, 626, 200, 978, 962, 678, 886, 28, 636, 907, 843, 17, 776, 977, 620, 369, 956, 761, 931, 292, 597, 607, 877, 502, 82, 79, 6, 778, 659, 787, 381, 75, 324, 396, 392, 289, 719, 816, 72, 4, 660, 775, 897, 600, 321, 429, 162, 458, 733, 598, 6, 191, 545, 739, 867, 145, 298, 220, 695, 458, 723, 532, 320, 785, 834, 539, 886, 399, 935, 23, 666, 318, 80, 50, 743, 47, 865, 900, 615, 747, 778, 483, 707, 787, 83, 47, 822, 428, 667, 225, 219, 375, 695, 131, 682, 618, 426, 836, 763, 632, 208, 645, 574, 787, 679, 28, 91, 580, 851, 167, 490, 491, 131, 483, 460, 386, 793, 865, 886, 886, 776, 816, 614, 390, 886, 885, 105, 795, 131, 97, 510, 695, 352, 191, 102, 42, 732, 787, 748, 549, 131, 328, 796, 11, 786, 131, 398, 826, 597, 386, 114, 353, 672, 992, 764, 174, 515, 776, 776, 678, 307, 642, 806, 580, 43, 924, 886, 972, 435, 52, 924, 507, 894, 271, 180, 488, 787, 379, 671, 318, 583, 837, 546, 269, 507, 360, 509, 576, 454, 131, 450, 944, 105, 248, 11, 510, 666, 444, 369, 816, 503, 945, 539, 223, 554, 222, 685, 886, 469, 992, 549, 748, 196, 365, 562, 776, 292, 82, 42, 928, 488, 546, 972, 766, 131, 483, 380, 716, 205, 861, 912, 942, 140, 574, 791, 851, 732, 614, 576, 886, 34, 336, 789, 822, 984, 425, 244, 773, 886, 123, 541, 705, 822, 798, 84, 603, 660, 850, 760, 976, 471, 546, 886, 792, 483, 858, 743, 664, 587, 250, 566, 906, 418, 141, 753, 562, 165, 944, 131, 244, 343, 464, 712, 266, 671, 890, 710, 612, 974, 847, 695, 335, 844, 2, 525, 846, 251, 972, 974, 106, 335, 81, 878, 403, 826, 472, 428, 322, 778, 131, 242, 865, 770, 113, 977, 816, 707, 444, 311, 250, 628, 17, 540, 686, 343, 810, 886, 667, 516, 753, 964, 130, 801, 766, 529, 695, 768, 900, 893, 738]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-02-14 00:54:38,244]\u001b[0m Trial 0 finished with value: 0.03400000000000001 and parameters: {'classifier': 'LogReg', 'logreg_c': 1.2166976553876506e-10}. Best is trial 0 with value: 0.03400000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-14 00:55:10,393]\u001b[0m Trial 1 finished with value: 0.03400000000000001 and parameters: {'classifier': 'LogReg', 'logreg_c': 2.2032212239411023e-06}. Best is trial 0 with value: 0.03400000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-14 00:55:56,711]\u001b[0m Trial 2 finished with value: 0.10700000000000001 and parameters: {'classifier': 'LogReg', 'logreg_c': 30.49364042784589}. Best is trial 2 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-14 00:56:35,231]\u001b[0m Trial 3 finished with value: 0.03400000000000001 and parameters: {'classifier': 'LogReg', 'logreg_c': 7.519448417128716e-06}. Best is trial 2 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-14 00:57:20,269]\u001b[0m Trial 4 finished with value: 0.10600000000000001 and parameters: {'classifier': 'LogReg', 'logreg_c': 1.6503068763663038}. Best is trial 2 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-14 00:57:46,752]\u001b[0m Trial 5 finished with value: 0.03400000000000001 and parameters: {'classifier': 'LogReg', 'logreg_c': 4.3936030142863146e-07}. Best is trial 2 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-14 00:58:30,815]\u001b[0m Trial 6 finished with value: 0.03400000000000001 and parameters: {'classifier': 'LogReg', 'logreg_c': 1.2807856653052942e-05}. Best is trial 2 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-14 00:58:37,974]\u001b[0m Trial 7 finished with value: 0.03400000000000001 and parameters: {'classifier': 'LogReg', 'logreg_c': 5.116611593040225e-09}. Best is trial 2 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-14 00:59:25,510]\u001b[0m Trial 8 finished with value: 0.11100000000000002 and parameters: {'classifier': 'LogReg', 'logreg_c': 14721.100769729941}. Best is trial 8 with value: 0.11100000000000002.\u001b[0m\n",
            "\u001b[32m[I 2021-02-14 00:59:27,013]\u001b[0m Trial 9 finished with value: 0.03400000000000001 and parameters: {'classifier': 'LogReg', 'logreg_c': 8.063489029396975e-10}. Best is trial 8 with value: 0.11100000000000002.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "score:  [0.12 0.11 0.11 0.13 0.09 0.09 0.03 0.15 0.12 0.1 ]\n",
            "Validation accuracy: 10.5\n",
            "Test accuracy: 0.513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pxdizTp8alH",
        "outputId": "4111f4da-dcfe-4eb6-8840-430f519a703c"
      },
      "source": [
        "pip show scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: scikit-learn\n",
            "Version: 0.22.2.post1\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: None\n",
            "Author-email: None\n",
            "License: new BSD\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: numpy, scipy, joblib\n",
            "Required-by: yellowbrick, umap-learn, textgenrnn, sklearn, sklearn-pandas, pynndescent, mlxtend, lucid, lightgbm, librosa, imbalanced-learn, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Du-ffuzkzgN",
        "outputId": "1090778d-585f-4c4c-d9e1-73ae4eaac168"
      },
      "source": [
        "pip install optuna"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/88/9c53460b97c61bce926dfe9dce51e4887c283416ff89ed30af0b73f44efa/optuna-2.5.0-py3-none-any.whl (287kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 18.3MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 18.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 30kB 10.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 92kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 102kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 112kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 122kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 133kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 143kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 153kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 163kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 174kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 184kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 194kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 204kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 215kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 225kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 235kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 245kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 256kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 266kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 276kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 296kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (1.0.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.23)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/53/daab5c96e22e9ed1c9f8ca4e3256e72213ade42d519b6254c32e59610967/alembic-1.5.4.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 7.3MB/s \n",
            "\u001b[?25hCollecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/8f/3c74fa4b6c3db1051b495385f5302fc5d5aa0f180d40ce3e9a13c82f8c82/cliff-3.6.0-py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/39/0230290df0519d528d8d0ffdfd900150ed24e0076d13b1f19e279444aab1/colorlog-4.7.2-py2.py3-none-any.whl\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/21/a2/21775c7343e7dd345e5e12cc7b8432e3d3edae2043b01bc2ecf38c432ef7/cmaes-0.8.1-py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (20.9)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/db/2d2d88b924aa4674a080aae83b59ea19d593250bfe5ed789947c21736785/Mako-1.1.4.tar.gz (479kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 14.5MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hCollecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
            "Collecting cmd2!=0.8.3,>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/8b/15061b32332bb35ea2a2f6263d0f616779d576e82739ec8e7fcf3c94abf5/cmd2-1.5.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 18.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->alembic->optuna) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from stevedore>=2.0.1->cliff->optuna) (3.4.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.3.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/4c/0b1d507ad7e8bc31d690d04b4f475e74c2002d060f7994ce8c09612df707/pyperclip-1.8.1.tar.gz\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->stevedore>=2.0.1->cliff->optuna) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->stevedore>=2.0.1->cliff->optuna) (3.7.4.3)\n",
            "Building wheels for collected packages: alembic, Mako, PrettyTable, pyperclip\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.5.4-py2.py3-none-any.whl size=156314 sha256=87c4f618cae626f2bb4f9eae12d32e977547775ccc5ef63e0b120f8eec17eca0\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/2d/ec/5a1b1e2363ed68392d292d215facf588d5448198edd8078bc1\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Mako: filename=Mako-1.1.4-py2.py3-none-any.whl size=75675 sha256=8ccc085af0ed85161b0b7063c2f8da7890c320f86bd94ed7ae11c5f05c0d6f34\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/10/d3/aeb26e20d19045e2a68e5d3cbb57432e11b5d9c92c99f98d47\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-cp36-none-any.whl size=13702 sha256=6963706e49e80fbe95c63bf23406218ef1c43fdbee18aca23ac5b6714fe0fff1\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.1-cp36-none-any.whl size=11120 sha256=2b8903484b426f2f3a7fb735aa71da121dc03e3afa4dea1f44ee2e8db38a4fed\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/10/3a/c830e9bb3db2c93274ea1f213a41fabde0d8cf3794251fad0c\n",
            "Successfully built alembic Mako PrettyTable pyperclip\n",
            "Installing collected packages: Mako, python-editor, alembic, pbr, stevedore, PrettyTable, pyperclip, colorama, cmd2, cliff, colorlog, cmaes, optuna\n",
            "  Found existing installation: prettytable 2.0.0\n",
            "    Uninstalling prettytable-2.0.0:\n",
            "      Successfully uninstalled prettytable-2.0.0\n",
            "Successfully installed Mako-1.1.4 PrettyTable-0.7.2 alembic-1.5.4 cliff-3.6.0 cmaes-0.8.1 cmd2-1.5.0 colorama-0.4.4 colorlog-4.7.2 optuna-2.5.0 pbr-5.5.1 pyperclip-1.8.1 python-editor-1.0.4 stevedore-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rqtTNF2NPFw",
        "outputId": "33dfb139-fa96-49f4-934d-04d0209a2f2a"
      },
      "source": [
        "from sklearn import datasets, linear_model\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "diabetes = datasets.load_diabetes()\r\n",
        "X = diabetes.data[:150]\r\n",
        "y = diabetes.target[:150]\r\n",
        "print('X: ', X)\r\n",
        "print('y: ', y)\r\n",
        "y = diabetes.target[:150]\r\n",
        "lasso = linear_model.Lasso()\r\n",
        "print(cross_val_score(lasso, X, y, cv=3))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X:  [[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990842\n",
            "  -0.01764613]\n",
            " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06832974\n",
            "  -0.09220405]\n",
            " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286377\n",
            "  -0.02593034]\n",
            " ...\n",
            " [-0.05637009 -0.04464164  0.09295276 ...  0.02545259  0.02605609\n",
            "   0.04034337]\n",
            " [-0.06000263  0.05068012  0.01535029 ... -0.00259226 -0.03075121\n",
            "  -0.0010777 ]\n",
            " [-0.04910502  0.05068012 -0.00512814 ...  0.07120998  0.06123791\n",
            "  -0.03835666]]\n",
            "y:  [151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
            " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
            " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
            "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
            "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
            "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
            "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
            "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
            " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
            "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
            " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126.]\n",
            "[0.33150734 0.08022311 0.03531764]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNryXVT-bI4O",
        "outputId": "22e0a5bb-eb28-4190-e764-c20a289bb6d8"
      },
      "source": [
        "pip install --pre --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/4a/c529159de5a417eb2f574941ccd9f937a47cafffaf1a3e485c6e2a8a4153/gensim-4.0.0b0-cp36-cp36m-manylinux1_x86_64.whl (24.0MB)\n",
            "\u001b[K     |████████████████████████████████| 24.0MB 163kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (4.1.2)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from gensim) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.0b0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIGBM4QwtaAn",
        "outputId": "15b86f6e-b122-45a3-ab1c-d0c64f2fb8ce"
      },
      "source": [
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}