{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paragraph_similarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Cpzo7rPDVpGRiYAGuvjvEmJGO9IcQELh",
      "authorship_tag": "ABX9TyPeH/b3HKUHnoGhuXlBXSM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hubertwel/paragraph-similarity/blob/main/paragraph-similarity/paragraph_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJr_9dXGpJ05",
        "outputId": "0d8d2a88-63a5-4097-eb29-79d97a55cf94"
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.parsing.preprocessing import preprocess_string\n",
        "from gensim.parsing.preprocessing import strip_tags, strip_multiple_whitespaces\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_short\n",
        "from gensim.parsing.preprocessing import strip_non_alphanum, split_alphanum\n",
        "from gensim.parsing.preprocessing import strip_punctuation\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.utils import RULE_KEEP, RULE_DISCARD, RULE_DEFAULT\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import os\n",
        "import smart_open\n",
        "import csv\n",
        "import collections\n",
        "import random\n",
        "import re\n",
        "import optuna\n",
        "\n",
        "def read_corpus(fname, tokens_only=False):\n",
        "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
        "        csv_reader = csv.DictReader(f, quoting=csv.QUOTE_ALL)\n",
        "        CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_multiple_whitespaces]\n",
        "        for i, row in enumerate(csv_reader):\n",
        "          line = row['x']\n",
        "          line = remove_urls(line)\n",
        "          line_list = preprocess_string(line, CUSTOM_FILTERS)\n",
        "          line = \" \".join(line_list)\n",
        "          tokens = simple_preprocess(line)\n",
        "          if tokens_only:\n",
        "            yield tokens\n",
        "          else:\n",
        "            # For training data, add tags\n",
        "            yield TaggedDocument(tokens, [i])\n",
        "\n",
        "def remove_urls(text):\n",
        "  text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', ' ', text, flags=re.MULTILINE)\n",
        "  return text\n",
        "\n",
        "def trim_rule(word, count, min_count):\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    # This rule is only used to prune vocabulary during the current method call,\n",
        "    # so that documents can be printed with stopwords and with words of any length\n",
        "    if ((word in stop_words) or (len(word) < 3)):\n",
        "        return utils.RULE_DISCARD  # throw out\n",
        "    else:\n",
        "        return utils.RULE_DEFAULT  # apply default rule, i.e. min_count\n",
        "\n",
        "# Define an objective function to be maximized\n",
        "def objective(trial):\n",
        "  # Optimize hyperparameters: \n",
        "    penalty = trial.suggest_categorical(\"penalty\", ['l1', 'l2'])\n",
        "    c = trial.suggest_float(\"C\", 5e-1, 15e-1, log=True)\n",
        "    fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
        "    intercept_scaling = trial.suggest_float(\"intercept_scaling\", 1e-1, 2e0, log=True)\n",
        "    clf = LogisticRegression(penalty=penalty, C=c, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, solver='liblinear', max_iter=300, class_weight='balanced', multi_class='auto')\n",
        "  # Scoring method:\n",
        "    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "    score = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=-1, scoring='accuracy')\n",
        "    accuracy = np.mean(score)\n",
        "    return accuracy\n",
        "\n",
        "def main():    \n",
        "  global X_train, y_train\n",
        "\n",
        "  # Set file names for train and test data\n",
        "  test_data_dir = os.path.join('/content/drive/MyDrive/Colab Notebooks/data/', 'gouvfr', 'CorpusRandomTwitter')\n",
        "  print(\"test_data_dir: %s\" % test_data_dir)\n",
        "  train_file = os.path.join(test_data_dir, 'randomtweets3.txt')\n",
        "  test_file = os.path.join(test_data_dir, 'randomtweets4.txt')\n",
        "\n",
        "  train_corpus = list(read_corpus(train_file, tokens_only=True))\n",
        "  train_corpus_tagged = list(read_corpus(train_file))\n",
        "  test_corpus = list(read_corpus(test_file, tokens_only=True))\n",
        "\n",
        "  print('train_corpus (the beginning): ', train_corpus[:2])\n",
        "  print('train_corpus length %d' % len(train_corpus))\n",
        "  print('train_corpus_tagged (the beginning): ', train_corpus_tagged[:2])\n",
        "  print('test_corpus (the beginning): ', test_corpus[:2])\n",
        "  print('test_corpus length %d' %len(test_corpus))\n",
        "  print()\n",
        "\n",
        "  # Build the model and its vocabulary\n",
        "  model = Doc2Vec(dm=0, vector_size=80, min_count=3, epochs=50, hs=1, dbow_words=1, trim_rule=trim_rule)\n",
        "  model.build_vocab(train_corpus_tagged)\n",
        "\n",
        "  # Train the model on the train corpus\n",
        "  model.train(train_corpus_tagged, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "  print()\n",
        "\n",
        "  # Assess the model on the train corpus\n",
        "  ranks = []\n",
        "  first_ranks = []\n",
        "  second_ranks = []\n",
        "  inferred_vectors = []\n",
        "  for doc_id in range(len(train_corpus_tagged)):\n",
        "    inferred_vector = model.infer_vector(train_corpus_tagged[doc_id].words)\n",
        "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "    # sanity check (self-similarity)\n",
        "    rank = [docid for docid, sim in sims].index(doc_id)\n",
        "    ranks.append(rank)\n",
        "    first_ranks.append(sims[0][0])\n",
        "    inferred_vectors.append(inferred_vector)\n",
        "        \n",
        "  print('sims: ', sims[:3])\n",
        "  print('ranks: ', ranks)\n",
        "  counter = collections.Counter(ranks)\n",
        "  print(counter)\n",
        "  print()\n",
        "\n",
        "  # Test the model with one random document\n",
        "  # Pick a random document from the test corpus and infer a vector from the model\n",
        "  doc_id = random.randint(0, len(test_corpus) - 1)\n",
        "  inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
        "  sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "  print('RANDOM TEST DOCUMENT ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
        "\n",
        "  # Compare and print the most/second-most/third-most/median/least similar documents from the train corpus\n",
        "  print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "  for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('THIRD-MOST', 2), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus_tagged[sims[index][0]].words)))\n",
        "\n",
        "  # Save the model\n",
        "  filename = get_tmpfile(\"parsim_doc2vec_model\")\n",
        "  model.save(filename)\n",
        "\n",
        "  # Assess the model on the independent data set (test corpus)\n",
        "  ranks_test = []\n",
        "  first_ranks_test = []\n",
        "  inferred_vectors_test = []\n",
        "  for doc_id in range(len(test_corpus)):\n",
        "    inferred_vector_test = model.infer_vector(test_corpus[doc_id])\n",
        "    sims_test = model.dv.most_similar([inferred_vector_test], topn=len(model.dv))\n",
        "    first_ranks_test.append(sims_test[0][0])\n",
        "    inferred_vectors_test.append(inferred_vector_test)\n",
        "\n",
        "  print('first_ranks_test: ', first_ranks_test)\n",
        "\n",
        "  # Prepare vectors for cross validatiom\n",
        "  tags_array_train = np.array(first_ranks)\n",
        "  vectors_2Darray_train = np.array(inferred_vectors)\n",
        "  tags_array_test = np.array(first_ranks_test)\n",
        "  vectors_2Darray_test = np.array(inferred_vectors_test)\n",
        "  y_train, X_train = tags_array_train, vectors_2Darray_train\n",
        "  y_test, X_test = tags_array_test, vectors_2Darray_test\n",
        "\n",
        "  # Create Optuna study\n",
        "  study = optuna.create_study(direction=\"maximize\")\n",
        "  study.optimize(objective, n_trials=30)\n",
        "  \n",
        "  # Cross validation\n",
        "  clf = LogisticRegression(penalty=study.best_params[\"penalty\"], C=study.best_params[\"C\"], fit_intercept=study.best_params[\"fit_intercept\"], intercept_scaling=study.best_params[\"intercept_scaling\"], solver='liblinear', max_iter=300, class_weight='balanced', multi_class='auto')\n",
        "  k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "  score = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=-1, scoring='accuracy')\n",
        "  print('score: ', score)\n",
        "  print('Validation accuracy: {}'.format(round(np.mean(score)*100, 3)))\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(\"Test accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred)))\n",
        "  \n",
        "main()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_data_dir: /content/drive/MyDrive/Colab Notebooks/data/gouvfr/CorpusRandomTwitter\n",
            "train_corpus (the beginning):  [['rt', 'americanidol', 'watch', 'as', 'ddlovato', 'gives', 'an', 'amazing', 'performance', 'of', 'her', 'single', 'stonecold', 'demionidol'], ['rt', 'dipti_varun', 'varun', 'dhawan', 'junaid', 'is', 'rookie', 'cop', 'and', 'heâ', 'kind', 'of', 'discovering', 'himself', 'as', 'the', 'movie', 'goes', 'on', 'varun_dvn', 'dishoom']]\n",
            "train_corpus length 1000\n",
            "train_corpus_tagged (the beginning):  [TaggedDocument(words=['rt', 'americanidol', 'watch', 'as', 'ddlovato', 'gives', 'an', 'amazing', 'performance', 'of', 'her', 'single', 'stonecold', 'demionidol'], tags=[0]), TaggedDocument(words=['rt', 'dipti_varun', 'varun', 'dhawan', 'junaid', 'is', 'rookie', 'cop', 'and', 'heâ', 'kind', 'of', 'discovering', 'himself', 'as', 'the', 'movie', 'goes', 'on', 'varun_dvn', 'dishoom'], tags=[1])]\n",
            "test_corpus (the beginning):  [['live', 'morning', 'news', 'weather', 'and', 'traffic', 'updates', 'fromâ', 'wdsu'], ['when', 'catch', 'fire', 'and', 'watch', 'over', 'you', 'like', 'the', 'sun']]\n",
            "test_corpus length 1000\n",
            "\n",
            "\n",
            "sims:  [(807, 0.9576311111450195), (677, 0.9548047184944153), (999, 0.949691891670227)]\n",
            "ranks:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 10, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 11, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 18, 0, 0, 11, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 19, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 0, 25, 0, 0, 0, 4, 0, 0, 0, 0, 0, 21, 0, 0, 18, 0, 0, 0, 28, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 26, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 10, 10, 0, 0, 0, 0, 0, 25, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 23, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 2, 0, 0, 14, 0, 0, 0, 28, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 33, 0, 0, 0, 0, 0, 22, 0, 0, 0, 0, 13, 0, 0, 0, 0, 1, 21, 0, 30, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 29, 0, 1, 1, 0, 0, 0, 24, 0, 0, 0, 0, 0, 0, 0, 3, 19, 0, 0, 0, 0, 1, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 24, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2]\n",
            "Counter({0: 888, 1: 44, 2: 10, 10: 5, 3: 5, 7: 5, 18: 4, 19: 4, 13: 3, 11: 3, 4: 3, 9: 2, 16: 2, 22: 2, 14: 2, 23: 2, 25: 2, 21: 2, 28: 2, 24: 2, 26: 1, 33: 1, 30: 1, 29: 1, 5: 1, 12: 1, 20: 1, 17: 1})\n",
            "\n",
            "RANDOM TEST DOCUMENT (688): «web development reading list jquery ux research and xss in ads»\n",
            "\n",
            "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow+w,d80,n5,hs,w5,mc3,s0.001,t3):\n",
            "\n",
            "MOST (390, 0.6889329552650452): «rt fcbarcelona arda added to fcbarcelona long list of goalscorers from around the world»\n",
            "\n",
            "SECOND-MOST (659, 0.6849079132080078): «rt awesomityfun list of people trust»\n",
            "\n",
            "THIRD-MOST (360, 0.674942135810852): «rt barcaukfans fc barcelona has long list of goalscorers from around the world»\n",
            "\n",
            "MEDIAN (416, 0.20587952435016632): «rt jrothenbergtv decision no one is happy with true mark of great leader the»\n",
            "\n",
            "LEAST (383, -0.12629464268684387): «thought jenas did well on bbcqt don get the sneering he representative of wider electorate who aren engulfed by day to day politics»\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-02-19 01:19:55,174]\u001b[0m A new study created in memory with name: no-name-f0a9fffc-0b33-45cb-abaa-486f9497e94b\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "first_ranks_test:  [391, 763, 542, 636, 436, 450, 349, 714, 238, 131, 676, 697, 814, 656, 275, 46, 582, 325, 87, 77, 142, 983, 208, 412, 340, 382, 964, 169, 594, 235, 529, 800, 647, 42, 494, 802, 145, 496, 262, 223, 545, 893, 686, 914, 50, 355, 671, 367, 76, 82, 106, 250, 535, 665, 347, 68, 413, 298, 389, 789, 621, 567, 82, 939, 64, 545, 766, 67, 682, 251, 781, 764, 96, 270, 516, 918, 285, 76, 131, 546, 994, 502, 399, 336, 503, 82, 294, 844, 642, 709, 87, 733, 221, 483, 768, 463, 131, 481, 147, 778, 634, 242, 131, 17, 317, 429, 141, 906, 657, 514, 131, 106, 95, 163, 797, 247, 117, 994, 593, 769, 647, 886, 636, 197, 544, 901, 131, 131, 463, 764, 225, 788, 939, 9, 576, 352, 557, 683, 438, 660, 457, 264, 775, 500, 715, 633, 177, 962, 201, 225, 748, 131, 507, 117, 390, 776, 277, 79, 87, 270, 707, 682, 399, 895, 352, 595, 259, 285, 383, 397, 621, 261, 330, 364, 720, 278, 223, 360, 775, 967, 874, 507, 82, 591, 139, 36, 909, 127, 426, 83, 452, 20, 925, 87, 285, 170, 234, 604, 430, 369, 795, 922, 986, 986, 514, 899, 100, 106, 834, 500, 22, 976, 95, 3, 396, 398, 840, 918, 950, 387, 683, 147, 716, 352, 89, 663, 822, 888, 17, 223, 168, 859, 886, 91, 818, 615, 473, 163, 894, 678, 763, 577, 204, 418, 912, 492, 851, 886, 874, 553, 936, 323, 814, 458, 287, 151, 550, 42, 407, 131, 925, 541, 133, 120, 680, 486, 69, 145, 458, 174, 698, 835, 150, 382, 131, 419, 10, 687, 488, 764, 876, 122, 195, 718, 949, 364, 85, 758, 417, 269, 29, 769, 666, 435, 696, 435, 437, 597, 570, 234, 234, 859, 234, 104, 151, 805, 961, 105, 131, 151, 553, 445, 760, 851, 722, 305, 964, 773, 998, 152, 961, 800, 743, 933, 467, 422, 600, 711, 613, 332, 861, 374, 182, 769, 235, 182, 131, 437, 594, 970, 383, 483, 634, 441, 655, 877, 566, 233, 28, 961, 261, 42, 103, 437, 254, 90, 578, 167, 151, 660, 660, 568, 457, 122, 131, 90, 288, 267, 28, 682, 392, 95, 312, 568, 643, 964, 106, 986, 131, 1, 264, 140, 886, 58, 414, 771, 125, 659, 320, 557, 19, 191, 546, 898, 670, 659, 60, 188, 131, 291, 340, 783, 604, 909, 272, 671, 405, 945, 733, 708, 390, 220, 453, 298, 330, 223, 335, 836, 342, 986, 51, 42, 954, 791, 512, 503, 886, 216, 131, 454, 925, 150, 332, 407, 131, 529, 513, 367, 733, 97, 169, 827, 483, 215, 207, 342, 453, 103, 188, 504, 919, 51, 558, 812, 733, 196, 366, 197, 924, 900, 516, 809, 360, 710, 100, 516, 841, 42, 396, 131, 811, 85, 483, 895, 296, 459, 657, 918, 131, 174, 857, 657, 213, 98, 126, 450, 481, 744, 107, 63, 773, 931, 926, 541, 987, 647, 439, 131, 526, 174, 842, 77, 772, 282, 885, 166, 619, 454, 553, 488, 181, 657, 861, 659, 859, 613, 981, 814, 922, 117, 189, 808, 41, 950, 886, 446, 55, 727, 310, 618, 609, 920, 570, 924, 747, 905, 103, 818, 294, 360, 886, 635, 696, 277, 257, 110, 341, 493, 819, 864, 970, 94, 437, 351, 501, 578, 594, 611, 763, 613, 11, 786, 131, 769, 131, 673, 689, 625, 326, 886, 405, 641, 967, 87, 99, 647, 861, 50, 259, 289, 581, 761, 132, 104, 311, 573, 972, 443, 270, 28, 364, 692, 496, 167, 617, 428, 822, 463, 551, 399, 482, 326, 206, 697, 565, 568, 483, 659, 44, 886, 622, 609, 575, 286, 886, 423, 427, 25, 733, 131, 904, 656, 682, 789, 213, 139, 929, 143, 87, 125, 65, 9, 953, 177, 69, 214, 916, 707, 131, 105, 886, 160, 591, 617, 131, 740, 80, 378, 454, 133, 825, 206, 428, 589, 202, 79, 651, 355, 131, 508, 14, 734, 647, 978, 962, 678, 886, 28, 90, 907, 843, 967, 776, 977, 620, 369, 956, 761, 931, 613, 638, 607, 336, 502, 496, 998, 6, 853, 659, 787, 381, 75, 324, 396, 392, 289, 719, 864, 72, 4, 660, 656, 629, 600, 321, 429, 162, 131, 733, 446, 6, 191, 545, 739, 551, 35, 298, 220, 695, 131, 723, 532, 473, 785, 834, 539, 886, 617, 186, 23, 666, 318, 80, 50, 743, 221, 865, 618, 615, 747, 778, 927, 707, 787, 917, 598, 822, 428, 667, 225, 856, 963, 695, 131, 682, 924, 426, 814, 763, 98, 208, 190, 570, 787, 679, 28, 91, 271, 851, 167, 256, 491, 131, 483, 131, 386, 793, 386, 886, 886, 776, 457, 775, 42, 886, 885, 105, 492, 131, 97, 510, 695, 819, 191, 102, 656, 857, 787, 748, 549, 131, 726, 796, 173, 786, 131, 398, 826, 161, 386, 114, 587, 841, 992, 698, 426, 515, 776, 776, 678, 307, 642, 806, 580, 43, 897, 886, 972, 106, 52, 79, 678, 894, 328, 180, 488, 787, 379, 315, 898, 25, 837, 546, 147, 678, 360, 824, 576, 454, 131, 450, 846, 105, 248, 173, 307, 666, 444, 369, 816, 503, 218, 614, 223, 468, 222, 685, 886, 207, 992, 549, 748, 196, 161, 562, 723, 613, 213, 42, 928, 131, 546, 972, 766, 131, 682, 380, 529, 205, 861, 912, 942, 140, 574, 791, 148, 873, 614, 974, 886, 34, 336, 789, 939, 984, 632, 82, 912, 886, 868, 895, 30, 831, 74, 84, 603, 660, 370, 760, 976, 471, 546, 886, 515, 111, 668, 743, 664, 587, 250, 566, 906, 235, 778, 616, 197, 257, 636, 131, 426, 343, 221, 444, 266, 905, 300, 644, 577, 463, 449, 403, 335, 794, 722, 525, 639, 191, 972, 974, 106, 335, 81, 972, 323, 131, 156, 91, 104, 778, 131, 242, 865, 859, 794, 73, 816, 707, 444, 311, 250, 628, 967, 540, 723, 934, 810, 82, 1, 769, 278, 143, 130, 831, 766, 714, 695, 890, 900, 271, 502]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-02-19 01:20:37,671]\u001b[0m Trial 0 finished with value: 0.10700000000000001 and parameters: {'penalty': 'l2', 'C': 0.6063930030273453, 'fit_intercept': True, 'intercept_scaling': 0.31635679552112306}. Best is trial 0 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:21:21,001]\u001b[0m Trial 1 finished with value: 0.10700000000000001 and parameters: {'penalty': 'l2', 'C': 0.978983813654303, 'fit_intercept': True, 'intercept_scaling': 0.15724816968031408}. Best is trial 0 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:21:57,071]\u001b[0m Trial 2 finished with value: 0.002 and parameters: {'penalty': 'l1', 'C': 0.6299645198821744, 'fit_intercept': True, 'intercept_scaling': 0.7136818201828395}. Best is trial 0 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:22:37,648]\u001b[0m Trial 3 finished with value: 0.10700000000000001 and parameters: {'penalty': 'l2', 'C': 0.5866806026384628, 'fit_intercept': False, 'intercept_scaling': 0.4342934399565916}. Best is trial 0 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:23:21,221]\u001b[0m Trial 4 finished with value: 0.10700000000000001 and parameters: {'penalty': 'l2', 'C': 1.4194555830843836, 'fit_intercept': True, 'intercept_scaling': 0.23736674870723162}. Best is trial 0 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:24:02,607]\u001b[0m Trial 5 finished with value: 0.10500000000000001 and parameters: {'penalty': 'l1', 'C': 0.5464552732982901, 'fit_intercept': False, 'intercept_scaling': 0.12280385855249935}. Best is trial 0 with value: 0.10700000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:24:44,554]\u001b[0m Trial 6 finished with value: 0.10900000000000001 and parameters: {'penalty': 'l2', 'C': 0.7412745891373863, 'fit_intercept': True, 'intercept_scaling': 1.1340904660114155}. Best is trial 6 with value: 0.10900000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:25:28,303]\u001b[0m Trial 7 finished with value: 0.10700000000000001 and parameters: {'penalty': 'l1', 'C': 0.7801656857050279, 'fit_intercept': False, 'intercept_scaling': 0.19349911659959093}. Best is trial 6 with value: 0.10900000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:26:15,382]\u001b[0m Trial 8 finished with value: 0.10700000000000001 and parameters: {'penalty': 'l1', 'C': 1.229151391812396, 'fit_intercept': False, 'intercept_scaling': 1.1639925751883926}. Best is trial 6 with value: 0.10900000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:26:58,927]\u001b[0m Trial 9 finished with value: 0.10800000000000001 and parameters: {'penalty': 'l2', 'C': 1.4472352987011552, 'fit_intercept': True, 'intercept_scaling': 0.4457675519593802}. Best is trial 6 with value: 0.10900000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:27:41,914]\u001b[0m Trial 10 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.8342538840618763, 'fit_intercept': True, 'intercept_scaling': 1.9360482623636328}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:28:24,282]\u001b[0m Trial 11 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.806577190713769, 'fit_intercept': True, 'intercept_scaling': 1.7836863941593153}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:29:08,068]\u001b[0m Trial 12 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.986021205554702, 'fit_intercept': True, 'intercept_scaling': 1.9468120351142713}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:29:51,490]\u001b[0m Trial 13 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.9220956671903598, 'fit_intercept': True, 'intercept_scaling': 1.8916877265361183}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:30:33,140]\u001b[0m Trial 14 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.7168691969043571, 'fit_intercept': True, 'intercept_scaling': 1.2546919193877233}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:31:17,147]\u001b[0m Trial 15 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 1.135077890131467, 'fit_intercept': True, 'intercept_scaling': 1.9280916894802496}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:31:59,984]\u001b[0m Trial 16 finished with value: 0.10800000000000001 and parameters: {'penalty': 'l2', 'C': 0.8269503159866268, 'fit_intercept': True, 'intercept_scaling': 0.7414186987973574}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:32:41,989]\u001b[0m Trial 17 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.7084795644303389, 'fit_intercept': True, 'intercept_scaling': 1.200158508981024}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:33:25,252]\u001b[0m Trial 18 finished with value: 0.10800000000000001 and parameters: {'penalty': 'l2', 'C': 1.1421755630174228, 'fit_intercept': True, 'intercept_scaling': 0.8327240572242945}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:34:08,491]\u001b[0m Trial 19 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 1.1479933613022186, 'fit_intercept': True, 'intercept_scaling': 1.5552125594837043}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:34:48,022]\u001b[0m Trial 20 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.508652032424459, 'fit_intercept': True, 'intercept_scaling': 1.346591369899984}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:35:29,958]\u001b[0m Trial 21 finished with value: 0.10800000000000001 and parameters: {'penalty': 'l2', 'C': 0.6801726054061722, 'fit_intercept': True, 'intercept_scaling': 0.9573668028066283}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:36:12,430]\u001b[0m Trial 22 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.8757288686093904, 'fit_intercept': True, 'intercept_scaling': 1.5226839937843313}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:36:52,148]\u001b[0m Trial 23 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.5133267588464032, 'fit_intercept': True, 'intercept_scaling': 1.5238204924844685}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:37:35,592]\u001b[0m Trial 24 finished with value: 0.10800000000000001 and parameters: {'penalty': 'l2', 'C': 1.2915438800062877, 'fit_intercept': True, 'intercept_scaling': 0.5920429743703342}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:38:18,693]\u001b[0m Trial 25 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.9973355806391796, 'fit_intercept': True, 'intercept_scaling': 1.500153846043383}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:39:01,444]\u001b[0m Trial 26 finished with value: 0.10800000000000001 and parameters: {'penalty': 'l2', 'C': 0.9101630505846228, 'fit_intercept': True, 'intercept_scaling': 0.9767223453290474}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:39:46,351]\u001b[0m Trial 27 finished with value: 0.10700000000000001 and parameters: {'penalty': 'l1', 'C': 0.9147970566683449, 'fit_intercept': False, 'intercept_scaling': 1.5898571049750414}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:40:29,661]\u001b[0m Trial 28 finished with value: 0.10800000000000001 and parameters: {'penalty': 'l2', 'C': 1.0392231313960159, 'fit_intercept': True, 'intercept_scaling': 0.6338960072324732}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n",
            "\u001b[32m[I 2021-02-19 01:41:10,741]\u001b[0m Trial 29 finished with value: 0.11000000000000001 and parameters: {'penalty': 'l2', 'C': 0.6602124296217698, 'fit_intercept': True, 'intercept_scaling': 1.9872976753827372}. Best is trial 10 with value: 0.11000000000000001.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "score:  [0.14 0.15 0.1  0.06 0.13 0.06 0.09 0.16 0.1  0.11]\n",
            "Validation accuracy: 11.0\n",
            "Test accuracy: 0.549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "-pxdizTp8alH",
        "outputId": "b07b9a64-9d7b-4bd7-9e80-72b05ac52a4c"
      },
      "source": [
        "pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/4c/6111b9a325f29527d7f262e2ee8c730d354b47a728d955e186dacad57a0d/scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl (22.2MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2MB 66.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.1 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Du-ffuzkzgN",
        "outputId": "34a22f6e-5f0c-4283-df31-164713a637e5"
      },
      "source": [
        "pip install optuna"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/88/9c53460b97c61bce926dfe9dce51e4887c283416ff89ed30af0b73f44efa/optuna-2.5.0-py3-none-any.whl (287kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.23)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/53/daab5c96e22e9ed1c9f8ca4e3256e72213ade42d519b6254c32e59610967/alembic-1.5.4.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.9MB/s \n",
            "\u001b[?25hCollecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/d6/7d9acb68a77acd140be7fececb7f2701b2a29d2da9c54184cb8f93509590/cliff-3.7.0-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (1.0.0)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/39/0230290df0519d528d8d0ffdfd900150ed24e0076d13b1f19e279444aab1/colorlog-4.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (20.9)\n",
            "Collecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/21/a2/21775c7343e7dd345e5e12cc7b8432e3d3edae2043b01bc2ecf38c432ef7/cmaes-0.8.1-py3-none-any.whl\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/db/2d2d88b924aa4674a080aae83b59ea19d593250bfe5ed789947c21736785/Mako-1.1.4.tar.gz (479kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 49.3MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n",
            "Collecting cmd2>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/8b/15061b32332bb35ea2a2f6263d0f616779d576e82739ec8e7fcf3c94abf5/cmd2-1.5.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 42.1MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.0.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->alembic->optuna) (1.15.0)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.4.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/4c/0b1d507ad7e8bc31d690d04b4f475e74c2002d060f7994ce8c09612df707/pyperclip-1.8.1.tar.gz\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2>=1.0.0->cliff->optuna) (20.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from PrettyTable>=0.7.2->cliff->optuna) (53.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2>=1.0.0->cliff->optuna) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2>=1.0.0->cliff->optuna) (3.7.4.3)\n",
            "Building wheels for collected packages: alembic, Mako, pyperclip\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.5.4-py2.py3-none-any.whl size=156314 sha256=67fc3dd74063c2f63a8b888bb536d653b4785eae239636887e13f1fd53aa6c3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/2d/ec/5a1b1e2363ed68392d292d215facf588d5448198edd8078bc1\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Mako: filename=Mako-1.1.4-py2.py3-none-any.whl size=75675 sha256=9753a97d1f3ee4bc6f67dd1817caa76ef7842ccf377169e5600af536addf1727\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/10/d3/aeb26e20d19045e2a68e5d3cbb57432e11b5d9c92c99f98d47\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.1-cp36-none-any.whl size=11120 sha256=f263b21a5e74ad54d2e32be10204fadca8ad9090fdf74ffebae387001600cf4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/10/3a/c830e9bb3db2c93274ea1f213a41fabde0d8cf3794251fad0c\n",
            "Successfully built alembic Mako pyperclip\n",
            "Installing collected packages: Mako, python-editor, alembic, colorama, pyperclip, cmd2, pbr, stevedore, cliff, colorlog, cmaes, optuna\n",
            "Successfully installed Mako-1.1.4 alembic-1.5.4 cliff-3.7.0 cmaes-0.8.1 cmd2-1.5.0 colorama-0.4.4 colorlog-4.7.2 optuna-2.5.0 pbr-5.5.1 pyperclip-1.8.1 python-editor-1.0.4 stevedore-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "PNryXVT-bI4O",
        "outputId": "e2e05abf-94d0-4d54-9827-32e9a04a69ce"
      },
      "source": [
        "pip install --pre --upgrade gensim"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/4a/c529159de5a417eb2f574941ccd9f937a47cafffaf1a3e485c6e2a8a4153/gensim-4.0.0b0-cp36-cp36m-manylinux1_x86_64.whl (24.0MB)\n",
            "\u001b[K     |████████████████████████████████| 24.0MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from gensim) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (4.1.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.19.5)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.0b0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIGBM4QwtaAn",
        "outputId": "15b86f6e-b122-45a3-ab1c-d0c64f2fb8ce"
      },
      "source": [
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}